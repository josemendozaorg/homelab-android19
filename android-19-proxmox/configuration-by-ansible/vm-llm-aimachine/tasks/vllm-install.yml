---
# vLLM Installation Tasks
# Install and configure vLLM inference server with systemd auto-start
# Implements Task 1.4 from Scenario 1

- name: Install Python 3 and pip
  ansible.builtin.apt:
    name:
      - python3
      - python3-pip
      - python3-venv
      - python3-full
    state: present
    update_cache: yes
  become: yes

- name: Create vLLM service user
  ansible.builtin.user:
    name: "{{ vllm_service_user }}"
    system: yes
    create_home: no
    shell: /bin/false
    comment: "vLLM inference server service user"
  become: yes

- name: Create vLLM virtual environment directory
  ansible.builtin.file:
    path: "{{ vllm_venv_path }}"
    state: directory
    owner: root
    group: root
    mode: '0755'
  become: yes

- name: Create Python virtual environment for vLLM
  ansible.builtin.command:
    cmd: python3 -m venv {{ vllm_venv_path }}
    creates: "{{ vllm_venv_path }}/bin/activate"
  become: yes

- name: Upgrade pip in virtual environment
  ansible.builtin.pip:
    name: pip
    state: latest
    virtualenv: "{{ vllm_venv_path }}"
  become: yes

- name: Create vLLM model directory
  ansible.builtin.file:
    path: "{{ vllm_model_dir }}"
    state: directory
    owner: "{{ vllm_service_user }}"
    group: "{{ vllm_service_user }}"
    mode: '0755'
  become: yes

- name: Install vLLM in virtual environment (with CUDA support)
  ansible.builtin.pip:
    name: vllm
    state: present
    extra_args: "--upgrade"
    virtualenv: "{{ vllm_venv_path }}"
  become: yes
  environment:
    # Ensure CUDA is detected during pip install
    CUDA_HOME: /usr/local/cuda

- name: Create vLLM cache directory
  ansible.builtin.file:
    path: "{{ vllm_model_dir }}/.cache"
    state: directory
    owner: "{{ vllm_service_user }}"
    group: "{{ vllm_service_user }}"
    mode: '0755'
  become: yes

- name: Create vLLM systemd service file
  ansible.builtin.copy:
    dest: /etc/systemd/system/vllm.service
    content: |
      [Unit]
      Description=vLLM OpenAI-Compatible Inference Server ({{ vllm_model_name }})
      After=network.target nvidia-persistenced.service
      Wants=nvidia-persistenced.service

      [Service]
      Type=simple
      User={{ vllm_service_user }}
      Group={{ vllm_service_user }}
      WorkingDirectory={{ vllm_model_dir }}
      ExecStart={{ vllm_venv_path }}/bin/python3 -m vllm.entrypoints.openai.api_server \
        --host {{ vllm_host }} \
        --port {{ vllm_port }} \
        --model {{ vllm_model_name }} \
        --gpu-memory-utilization {{ vllm_gpu_memory_utilization }} \
        --download-dir {{ vllm_model_dir }}
      Restart=on-failure
      RestartSec=10s
      Environment="CUDA_VISIBLE_DEVICES=0"
      Environment="HF_HOME={{ vllm_model_dir }}"
      Environment="HOME={{ vllm_model_dir }}"
      Environment="XDG_CACHE_HOME={{ vllm_model_dir }}/.cache"
      Environment="VLLM_CACHE_ROOT={{ vllm_model_dir }}/.cache"

      [Install]
      WantedBy=multi-user.target
    owner: root
    group: root
    mode: '0644'
  become: yes
  notify: Restart vLLM service

- name: Reload systemd daemon
  ansible.builtin.systemd:
    daemon_reload: yes
  become: yes

- name: Enable and start vLLM service
  ansible.builtin.systemd:
    name: vllm
    enabled: true
    state: started
  become: yes

- name: Wait for vLLM API to become available
  ansible.builtin.uri:
    url: "http://{{ vllm_host }}:{{ vllm_port }}/health"
    method: GET
    status_code: 200
  register: vllm_health_check
  until: vllm_health_check.status == 200
  retries: 12
  delay: 5
  failed_when: false

- name: Verify vLLM service is running
  ansible.builtin.systemd:
    name: vllm
    state: started
  check_mode: yes
  register: vllm_service_status

- name: Display vLLM service status
  ansible.builtin.debug:
    msg: |
      âœ… vLLM Installation Complete
      Service: {{ 'running' if vllm_service_status.status is defined else 'installed' }}
      API Endpoint: http://{{ vllm_host }}:{{ vllm_port }}
      Health Endpoint: http://{{ vllm_host }}:{{ vllm_port }}/health
      Model Directory: {{ vllm_model_dir }}
      Auto-start: Enabled
