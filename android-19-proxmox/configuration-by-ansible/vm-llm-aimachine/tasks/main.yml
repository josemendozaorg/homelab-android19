---
# vm-llm-aimachine Role
# GPU-accelerated Ubuntu Server VM for AI/LLM workloads (vLLM, Ollama)

- name: Display VM configuration from catalog
  ansible.builtin.debug:
    msg: |
      üöÄ Configuring GPU-accelerated AI/LLM VM

      üìã VM Configuration (from infrastructure-catalog.yml):
      - VM ID: {{ vm_config.id | default('140') }}
      - VM Name: {{ vm_config.name }}
      - IP Address: {{ vm_config.ip }}
      - CPU: {{ vm_config.resources.cores }} cores
      - RAM: {{ (vm_config.resources.memory / 1024) | round(1) }}GB
      - Disk: {{ vm_config.resources.disk }}GB
      - GPU Passthrough: {{ vm_config.gpu_passthrough.enabled | default(false) }}
      {% if vm_config.gpu_passthrough.enabled | default(false) %}
      - GPU Device ID: {{ vm_config.gpu_passthrough.device_id }}
      {% endif %}

- name: Include GPU passthrough configuration
  include_tasks: gpu-passthrough.yml
  tags: [gpu, passthrough]
  vars:
    gpu_device_id: "{{ vm_config.gpu_passthrough.device_id }}"
    gpu_passthrough_enabled: true  # Always configure GPU passthrough (catalog.enabled is for Terraform only)

- name: Include DNS configuration tasks
  include_tasks: dns-configure.yml
  tags: [dns, network]

- name: Include NVIDIA driver installation tasks
  include_tasks: nvidia-drivers.yml
  tags: [nvidia, drivers, gpu]

- name: Include GPU Power Management tasks
  include_tasks: power-management.yml
  tags: [power, nvidia, ollama]

- name: Include Node Exporter
  include_tasks: ../../common/tasks/install-node-exporter.yml
  tags: [monitoring, node_exporter]

- name: Include Docker Installation
  include_tasks: ../../common/tasks/install-docker.yml
  tags: [monitoring, docker]

- name: Include NVIDIA Docker Support
  include_tasks: ../../common/tasks/install-nvidia-docker.yml
  tags: [monitoring, docker, nvidia]

- name: Include GPU Monitoring (DCGM)
  include_tasks: gpu-monitoring.yml
  tags: [monitoring, gpu]

- name: Include vLLM installation tasks
  include_tasks: vllm-install.yml
  tags: [vllm, ai, llm]

- name: Include Ollama installation tasks
  include_tasks: ollama-install.yml
  tags: [ollama, ai, llm]

- name: Include Ollama model management tasks
  include_tasks: ollama-models.yml
  tags: [ollama, models, ai, llm]
  when: ollama_pull_models_on_deploy | default(false)

- name: Display deployment summary
  ansible.builtin.debug:
    msg: |
      ‚úÖ AI/LLM VM Configuration Complete

      üîß Installed Components:
      - NVIDIA Drivers: {{ nvidia_driver_preference }}-kernel modules
      - vLLM API Server: http://vllm.homelab:{{ vllm_port }}
      - Ollama: http://ollama.homelab:{{ ollama_port }}

      üìÅ Model Storage:
      - vLLM models: {{ vllm_model_dir }}
      - Ollama models: {{ ollama_model_dir }}

      üöÄ Services Auto-start: {{ vllm_service_enabled and ollama_service_enabled }}

      ‚úÖ Ready for AI/LLM workloads!
